{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/dbamman/nlp23/blob/main/HW2/HW_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WlFS_v7TpRHe"
   },
   "source": [
    "# Homework 2: PyTorch and Self-Attention\n",
    "\n",
    "In this homework, you will begin exploring PyTorch, a neural network library that will be used throughout the remainder of the semester.  \n",
    "\n",
    "The PDF file for instructions can be found [here](https://github.com/dbamman/nlp23/blob/main/HW2/HW2.pdf). \n",
    "\n",
    "You can toggle the outline on the left hand side to jump around sections more easily.\n",
    "\n",
    "**Due date**: Tuesday February 7 at 11:59 PM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wyY0yl1Jpf2o"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "from collections import Counter\n",
    "\n",
    "# Sets random seeds for reproducibility\n",
    "seed=159259\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s7DVjxeq_-OF",
    "outputId": "0a25126b-d6a2-4a22-d91a-f2a5e90c4117"
   },
   "outputs": [],
   "source": [
    "!python -m nltk.downloader punkt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jlYfHZwlQXA_"
   },
   "source": [
    "When looking up pytorch documentation, it may be useful to know which version of torch you are running.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qUdEHON5lybF",
    "outputId": "ea13fd6e-7d82-4ca7-ed44-0173df878e17"
   },
   "outputs": [],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8xrbyc1flKp_"
   },
   "source": [
    "## **IMPORTANT**: GPU is not enabled by default\n",
    "\n",
    "You must switch runtime environments if your output of the next block of code has an error saying \"ValueError: Expected a cuda device, but got: cpu\"\n",
    "\n",
    "Go to Runtime > Change runtime type > Hardware accelerator > GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mRy4VWrvkCP6",
    "outputId": "9c3d4ef5-cf25-410f-e23f-4a79d5db9813"
   },
   "outputs": [],
   "source": [
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Running on {}\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GmhyEWPVCrNi"
   },
   "source": [
    "## Deliverable 1: PyTorch and FFNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XyARzkPKmUlR"
   },
   "source": [
    "### Data Processing\n",
    "\n",
    "Let's begin by loading our datasets and the 50-dimensional GLoVE word embeddings.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l_ZZQsGwH5vj",
    "outputId": "291e74f4-8ed2-44b2-b69b-548d25720f0d"
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/dbamman/nlp23/main/HW2/train.txt\n",
    "!wget https://raw.githubusercontent.com/dbamman/nlp23/main/HW2/dev.txt\n",
    "!wget https://raw.githubusercontent.com/dbamman/nlp23/main/HW2/glove.6B.50d.50K.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vC5tWWn2mWhH"
   },
   "outputs": [],
   "source": [
    "training_file, dev_file=\"train.txt\", \"dev.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q_vLcPzzIxDw"
   },
   "outputs": [],
   "source": [
    "labels={'pos': 0, 'neg': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sNb4H1auI4lA"
   },
   "outputs": [],
   "source": [
    "def read_embeddings(filename, vocab_size=50000):\n",
    "    \"\"\"\n",
    "    Utility function, loads in the `vocab_size` most common embeddings from `filename`\n",
    "    \n",
    "    Arguments:\n",
    "    - filename:     path to file\n",
    "                    automatically infers correct embedding dimension from filename\n",
    "    - vocab_size:   maximum number of embeddings to load\n",
    "\n",
    "    Returns \n",
    "    - embeddings:   torch.FloatTensor matrix of size (vocab_size x word_embedding_dim)\n",
    "    - vocab:        dictionary mapping word (str) to index (int) in embedding matrix\n",
    "    \"\"\"\n",
    "\n",
    "    # get the embedding size from the first embedding\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        word_embedding_dim=len(file.readline().split(\" \")) - 1\n",
    "\n",
    "    vocab={}\n",
    "\n",
    "    embeddings=np.zeros((vocab_size, word_embedding_dim))\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        for idx, line in enumerate(file):\n",
    "            if idx + 2 >= vocab_size:\n",
    "                break\n",
    "            cols=line.rstrip().split(\" \")\n",
    "            val=np.array(cols[1:])\n",
    "            word=cols[0]\n",
    "            embeddings[idx + 2]=val\n",
    "            vocab[word]=idx + 2\n",
    "    \n",
    "    # a FloatTensor is a multidimensional matrix\n",
    "    # that contains 32-bit floats in every entry\n",
    "    # https://pytorch.org/docs/stable/tensors.html\n",
    "    return torch.FloatTensor(embeddings), vocab\n",
    "\n",
    "def get_batches(x, y, xType, batch_size=12):\n",
    "    batches_x=[]\n",
    "    batches_y=[]\n",
    "    for i in range(0, len(x), batch_size):\n",
    "        batches_x.append(xType(x[i:i+batch_size]))\n",
    "        batches_y.append(torch.LongTensor(y[i:i+batch_size]))    \n",
    "    return batches_x, batches_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PrBHMiLPIOKB"
   },
   "source": [
    "### Demo: Logistic regression\n",
    "\n",
    "First, let's code up Logistic Regression in PyTorch so you can see how the general framework works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgaDKtOrc10l"
   },
   "source": [
    "#### Average Embedding Representation\n",
    "Let's train a logistic regression classifier where the input is the average GloVe embedding for all words in a review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0YgU4027luO3"
   },
   "outputs": [],
   "source": [
    "def read_avg_glove_embeddings(filename, vocab, embs, labels):\n",
    "    \"\"\"\n",
    "    Utility function, loads in texts `filename` and looks up the static embeddings\n",
    "    \n",
    "    Arguments:\n",
    "    - filename:     path to file\n",
    "    - vocab:        vocab file of e.g. GloVe\n",
    "\n",
    "    Returns \n",
    "    - embeddings:   torch.FloatTensor matrix of size (vocab_size x word_embedding_dim)\n",
    "    - vocab:        dictionary mapping word (str) to index (int) in embedding matrix\n",
    "    \"\"\"\n",
    "    data, data_labels=[], []\n",
    "    with open(filename) as file:\n",
    "        for line in file:\n",
    "            avg_emb=np.zeros(50)\n",
    "            cols=line.rstrip().split(\"\\t\")\n",
    "            idd=cols[0]\n",
    "            label=cols[1]\n",
    "            review=cols[2]\n",
    "            words=nltk.word_tokenize(review)\n",
    "            avg_counter=0.\n",
    "            for word in words:\n",
    "                word=word.lower()\n",
    "                if word in vocab:\n",
    "                    avg_emb += embs[vocab[word]].numpy()\n",
    "                    avg_counter += 1.\n",
    "            avg_emb /= avg_counter\n",
    "            data.append(avg_emb)\n",
    "            data_labels.append(labels[label])\n",
    "    return data, data_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JYb1iVsqb0Le"
   },
   "outputs": [],
   "source": [
    "embs, glove_vocab=read_embeddings(\"glove.6B.50d.50K.txt\")\n",
    "avg_train_x, avg_train_y=read_avg_glove_embeddings(training_file, glove_vocab, embs, labels)\n",
    "avg_dev_x, avg_dev_y=read_avg_glove_embeddings(dev_file, glove_vocab, embs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-FeYYf7-c01Z"
   },
   "outputs": [],
   "source": [
    "# ignore \"UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow.\"\n",
    "# We avoid Dataset etc. objects to simplify the code -- make sure you can understand what each of those lines does!\n",
    "avg_trainX, avg_trainY=get_batches(avg_train_x, avg_train_y, xType=torch.FloatTensor)\n",
    "avg_devX, avg_devY=get_batches(avg_dev_x, avg_dev_y, xType=torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fmqx6zFolx_L"
   },
   "source": [
    "### Question 1: PyTorch and embeddings (writeup)\n",
    "\n",
    "Here, you can see that a PyTorch implementation of logistic regression is offered for your reference. \n",
    "Study this script carefully: start with how we load and access the GloVe embeddings, and the most critical ingredients of a neural net in PyTorch include a class that defines the architecture (pay attention to the `forward` method), an optimizer (`torch.optim.Adam`), and a loss function, `torch.nn.CrossEntropyLoss()`, which combines the $\\mathrm{softmax}$ function `torch.nn.LogSoftmax()` and negative log-likelihood `torch.nn.NLLLoss()` (see [documentation](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)).\n",
    "\n",
    "**A notable difference from HW1 here is that the input of this model is the average of GloVe embeddings for all words in a movie review.\n",
    "What is the difference between averaged embeddings and BoW? What are the advantages or disadvantages of each? \n",
    "Discuss in no more than 100 words.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "392D8YLfI_K3"
   },
   "outputs": [],
   "source": [
    "# Models are usually implemented as classes in PyTorch; you need to pass nn.Module to inherit the base class\n",
    "class LogisticRegressionClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        # __init__ is what gets called when you instantiate the class\n",
    "        # and here you need to pass the arguments `input_dim` and `outpt_dim` when you do.\n",
    "        # As you can see in the next cell, a model instance is instantiated with\n",
    "        #     logreg=LogisticRegressionClassifier(input_dim, output_dim)\n",
    "        # and we pass the length of reviews -- avg_trainX[0].shape[1]\n",
    "        # and the size of labels -- len(labels) as arguments.        \n",
    "        super().__init__()\n",
    "        # this is how you define a linear layer -- \n",
    "        # remember to add \"self\" if you want the variable to be visible to the whole model\n",
    "        # (which we do)\n",
    "        self.linear=torch.nn.Linear(input_dim, output_dim)\n",
    "    def forward(self, input): \n",
    "        # The forward() method defines how your input tensors are processed.\n",
    "        # Don't change the name!\n",
    "        # It's conventional to name it forward (useful distinction from backward passes), and \n",
    "        # in fact you can omit the .forward call and just say \n",
    "        #    logreg(x)\n",
    "        # as opposed to \n",
    "        #    logreg.forward(x)        \n",
    "        h=self.linear(input)\n",
    "        return h\n",
    "    def evaluate(self, x, y):\n",
    "        # we use this method to evaluate model performance on dev sets\n",
    "        # if this all seems overwhelming, just focus on the first two methods for now.\n",
    "        # The important bits are self.eval() and no_grad(), which freeze the parameters \n",
    "        # so they won't keep updating during the evaluation process.\n",
    "        # (We don't want to cheat so should update the parameters only during training.)\n",
    "        self.eval()\n",
    "        corr=0.\n",
    "        total=0.\n",
    "        with torch.no_grad():\n",
    "            for x, y in zip(x, y):\n",
    "                x, y=x.to(device), y.to(device)\n",
    "                y_preds=self.forward(x)\n",
    "                for idx, y_pred in enumerate(y_preds):\n",
    "                    prediction=torch.argmax(y_pred)\n",
    "                    if prediction == y[idx]:\n",
    "                        corr += 1.\n",
    "                    total += 1                          \n",
    "        return corr/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76zFy8aqbtil"
   },
   "source": [
    "A complete training loop is here for your reference. There's nothing for you to implement here but you might want to make sure you understand the standard procedure. Remember that you **do not** have to finish training; none of your answers will depend on the model accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Duzn0vCrdR5X"
   },
   "outputs": [],
   "source": [
    "logreg=LogisticRegressionClassifier(avg_trainX[0].shape[1], len(labels)).to(device)\n",
    "optimizer=torch.optim.Adam(logreg.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "cross_entropy=nn.CrossEntropyLoss()\n",
    "losses=[]\n",
    "\n",
    "num_labels=len(labels)\n",
    "\n",
    "patience=10\n",
    "max_dev_accuracy=0\n",
    "patience_counter=0\n",
    "\n",
    "for epoch in range(200):\n",
    "    logreg.train()\n",
    "    \n",
    "    for x, y in zip(avg_trainX, avg_trainY):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_pred=logreg.forward(x)\n",
    "        loss=cross_entropy(y_pred.view(-1, num_labels), y.view(-1))\n",
    "        losses.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    dev_accuracy=logreg.evaluate(avg_devX, avg_devY)\n",
    "    \n",
    "    # check if the dev accuracy is the best seen so far\n",
    "    if dev_accuracy > max_dev_accuracy:\n",
    "        max_dev_accuracy=dev_accuracy\n",
    "        patience_counter=0\n",
    "    \n",
    "    patience_counter+=1\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print(\"Epoch %s, dev accuracy: %.3f\" % (epoch, dev_accuracy))\n",
    "    if patience_counter >= patience:\n",
    "        print(\"Stopping training; no improvement on dev data after %s epochs\" % patience)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cn3r2ZqxNjFN"
   },
   "source": [
    "### Question 2: FFNN (TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNUg5NYrm8K0"
   },
   "source": [
    "For this question, we want to add a hidden layer to the logistic regression classifier above. Implement Eqn. (7.13) in J&M SLP3 and let the non-linearity $g$ be $\\mathrm{tanh}$. Your implementation should be similar to the `LogisticRegressionClassifier` above. Let's pick $20$ for the size of the hidden layer -- this is provided in the `__init__()` function below (`hidden_dim=20`), so you don't need to worry about it.\n",
    "\n",
    "Note that in the J&M terminology, a \"two-layer\" network has one hidden layer, which is what you will be implementing. You should fill in the parts between \"`#BEGIN SOLUTION`\" and \"`#END SOLUTION`\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qk0-ptdEYPsH"
   },
   "outputs": [],
   "source": [
    "class FFNN(nn.Module):\n",
    "    # BEGIN SOLUTION\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        hidden_dim=20\n",
    "        # complete the two lines below\n",
    "        self.linear= ... \n",
    "        self.fc= ...\n",
    "        self.tanh=torch.nn.Tanh()\n",
    "    \n",
    "    def forward(...):\n",
    "        raise NotImplementedError\n",
    "    # END SOLUTION\n",
    "\n",
    "    def evaluate(self, x, y):\n",
    "        self.eval()\n",
    "        corr=0.\n",
    "        total=0.\n",
    "        with torch.no_grad():\n",
    "            for x, y in zip(x, y):\n",
    "                x, y=x.to(device), y.to(device)\n",
    "                y_preds=self.forward(x)\n",
    "                for idx, y_pred in enumerate(y_preds):\n",
    "                    prediction=torch.argmax(y_pred)\n",
    "                    if prediction == y[idx]:\n",
    "                        corr += 1.\n",
    "                    total += 1                          \n",
    "        return corr/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_sK7bP2wR96j"
   },
   "outputs": [],
   "source": [
    "ffnn_classifier=FFNN(avg_trainX[0].shape[1], len(labels)).to(device)\n",
    "optimizer=torch.optim.Adam(ffnn_classifier.parameters(), lr=0.001, weight_decay=0)\n",
    "cross_entropy=nn.CrossEntropyLoss()\n",
    "losses=[]\n",
    "\n",
    "num_labels=len(labels)\n",
    "\n",
    "patience=30\n",
    "max_dev_accuracy=0\n",
    "patience_counter=0\n",
    "\n",
    "for epoch in range(200):\n",
    "    ffnn_classifier.train()\n",
    "    \n",
    "    for x, y in zip(avg_trainX, avg_trainY):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_pred=ffnn_classifier.forward(x)\n",
    "        loss=cross_entropy(y_pred.view(-1, num_labels), y.view(-1))\n",
    "        losses.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    dev_accuracy=ffnn_classifier.evaluate(avg_devX, avg_devY)\n",
    "    \n",
    "    # check if the dev accuracy is the best seen so far\n",
    "    if dev_accuracy > max_dev_accuracy:\n",
    "        max_dev_accuracy=dev_accuracy\n",
    "        patience_counter=0\n",
    "    \n",
    "    patience_counter+=1\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print(\"Epoch %s, dev accuracy: %.3f\" % (epoch, dev_accuracy))\n",
    "    if patience_counter >= patience:\n",
    "        print(\"Stopping training; no improvement on dev data after %s epochs\" % patience)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IYz8NO-4O49b"
   },
   "source": [
    "## Deliverable 2: Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8EHx_Vkfn39v"
   },
   "source": [
    "The self-attention mechanism is often thought of as one of the most transformative ideas in modern NLP.\n",
    "Its full form in Transformer, as introduced in \"Attention is All You Need\" (NIPS 2017) is rather involved.\n",
    "This deliverable aims to prepare you for it. \n",
    "\n",
    "We will start with the simplest form of self-attention: scaled dot-product self-attention. The goal is to try to understand the roles that query, key, and value vectors play in attending to the input sequence: \n",
    "conceptually, what do they aim to achieve and improve on? How do you code this in Python?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ySzttdlioFrD"
   },
   "source": [
    "### Question 3: the concept of self-attention (writeup only)\n",
    "\n",
    "Self-attention may be one of those less intuitive concepts you have encountered in this course. \n",
    "\n",
    "In the era of ChatGPT, you might be curious if it can *tutor* you and answer your questions about a difficult concept in NLP.\n",
    "\n",
    "**This question is your chance to play around with it and start thinking about the power and limitations of such tools. \n",
    "First, make sure that you have worked through $\\S$10.1 in SLP 3 and the relevant lecture on self-attention.\n",
    "Then, you can go to https://chat.openai.com/chat, ask ChatGPT questions, and most importantly, evaluate its answers.\n",
    "Here's an example prompt:**\n",
    "\n",
    "> Why do we need the self-attention mechanism in NLP?\n",
    "\n",
    "**It yields the following answer:**\n",
    "\n",
    "> Self-attention is a mechanism in deep learning models for processing sequences, where each element in a sequence attends to all other elements in a weight-based manner to compute its representation. This enables the model to dynamically weigh the importance of different elements in the sequence for the current task, and allows for more context-aware representations compared to traditional recurrent neural networks.\n",
    "\n",
    "**Is this correct? Substantiate your reasoning in less than 100 words.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C7_z2cr8pThO"
   },
   "source": [
    "### Intition of attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qzu3s-xilmwO"
   },
   "source": [
    "First, let's go through the basics of implementing the steps in attention outside of any model.  We'll do that in Numpy.  Using the example from lecture, let's assume we have three sets of parameters $W^Q$, $W^K$ and $W^V$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CEFF55BQla6d",
    "outputId": "196ae838-c2db-4a31-92aa-6e62bbc2cc4e"
   },
   "outputs": [],
   "source": [
    "query_key_size=37\n",
    "input_embedding_size=2\n",
    "\n",
    "Wq=np.random.rand(input_embedding_size, query_key_size)\n",
    "Wk=np.random.rand(input_embedding_size, query_key_size)\n",
    "Wv=np.random.rand(input_embedding_size, input_embedding_size)\n",
    "\n",
    "print(Wq.shape, Wk.shape, Wv.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DlPgkm1Tn-H6"
   },
   "source": [
    "Let's also assume we have an input sentence that's 5 tokens long; each token is represented as an embedding of length `input_embedding_size` (here, $2$).  That 5-word sentence, then, is represented as as $5 \\times 2$ matrix `sent`.  If we multiply `sent` by $W^Q$, the result is a $5 \\times 37$ query matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yXFA1SJKmI_w",
    "outputId": "1a3df968-a32b-41e3-829d-6d124ed6baef"
   },
   "outputs": [],
   "source": [
    "sent=np.random.rand(5, input_embedding_size)\n",
    "key=sent @ Wq\n",
    "\n",
    "print(sent.shape, key.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4nZeiywDojyk"
   },
   "source": [
    "Now let's also show how to perform the softmax operation on a matrix.  Remember that the softmax function normalizes over a set of values $x = [x_1, \\ldots, x_n]$ such that each  $0 \\le x_i \\le 1$  and the sum of $x$ = 1.  Here, we have a $15 \\times 5$ matrix $m$; if we perform the softmax over the columns of $m$ (`axis=1`), each row will sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-wezc38AmUQa",
    "outputId": "015c1bdf-5840-4c9c-9fe5-1e1b63e5a14d"
   },
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "test_mamtrix=np.random.rand(15,5)\n",
    "print(f\"**test_mamtrix**:\\n {test_mamtrix}\")\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "\n",
    "# for a 2D matrix, axis=1 normalizes across the *columns*\n",
    "output=softmax(test_mamtrix, axis=1)\n",
    "print(f\"**output**:\\n {output}\")\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "# If we sum along the columns, each row should sum to 1\n",
    "print(f\"**sum of each row**:\")\n",
    "print(np.sum(output, axis=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ses_tWyuroZk"
   },
   "source": [
    "### Question 4: Scaled dot product attention in Numpy (TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dFmdQzqfpNYP"
   },
   "source": [
    "(TODO).  From all of this, you have the building blocks for implementing attention (outside of any model).  Do so here by filling out the attention function below.  Recall from lecture that attention given a query vector $Q$, key vector $K$ and value vector $V$ is given by the following equation:\n",
    "\n",
    "$$\\mathrm{Attention}({Q}, {K}, {V}) = \\mathrm{softmax}\\left(\\frac{{Q}{K}^\\top}{\\sqrt{d_k}}\\right){V}$$\n",
    "\n",
    "You will calculate ${Q}$, ${K}$, ${V}$ within the body of this function. The sole required argument to this function should be a 2D matrix $\\in \\mathbb{R}^{n~\\times~ \\textrm{input_embedding_size}}$ for any arbitrary $n$ (that is, corresponding to a sentence of arbitrary length).  It should return a matrix of that same exact size that is the output of that attention process over the input, given the parameters specified below. $d_k$ here is the size of the key vector (`query_key_size=37`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KJxi10sRm6cA",
    "outputId": "afa97544-edef-438e-e6d8-ca3f3bb52129"
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "query_key_size=37\n",
    "input_embedding_size=2\n",
    "\n",
    "Wq=np.random.rand(input_embedding_size, query_key_size)\n",
    "Wk=np.random.rand(input_embedding_size, query_key_size)\n",
    "Wv=np.random.rand(input_embedding_size, input_embedding_size)\n",
    "\n",
    "print(Wq.shape, Wk.shape, Wv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rIz14Cp0fkEQ"
   },
   "outputs": [],
   "source": [
    "def attention(input, input_embedding_size=2, query_key_size=37):\n",
    "    Wq=np.random.rand(input_embedding_size, query_key_size)\n",
    "    Wk=np.random.rand(input_embedding_size, query_key_size)\n",
    "    Wv=np.random.rand(input_embedding_size, input_embedding_size)\n",
    "\n",
    "    # BEGIN SOLUTION\n",
    "    raise NotImplementedError\n",
    "    # END SOLUTION\n",
    "    assert input.shape == output.shape\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iSWCuGOeO9Bm"
   },
   "source": [
    "## Question 5: Scaled dot product attention in PyTorch (TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data prep time again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HhCwiL2wasYv"
   },
   "outputs": [],
   "source": [
    "def read_data_as_embeddings(filename, vocab, labels, save_texts=False):\n",
    "    \"\"\"\n",
    "    Utility function, loads in texts `filename` and looks up the static embeddings\n",
    "    \n",
    "    Arguments:\n",
    "    - filename:     path to file\n",
    "    - vocab:        vocab file of e.g. GloVe\n",
    "    - labels:       label mapping\n",
    "    - save_texts:   whether to store the original texts\n",
    "\n",
    "    Returns \n",
    "    - embeddings:   torch.FloatTensor matrix of size (vocab_size x word_embedding_dim)\n",
    "    - vocab:        dictionary mapping word (str) to index (int) in embedding matrix\n",
    "    \"\"\"\n",
    "\n",
    "    PAD_INDEX=0             # reserved for padding words\n",
    "    UNKNOWN_INDEX=1         # reserved for unknown words\n",
    "    SEP_INDEX=2\n",
    "\n",
    "    texts=[]\n",
    "    data, data_labels=[], []\n",
    "\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            cols=line.split(\"\\t\")\n",
    "            idd=cols[0]\n",
    "            label=cols[1]\n",
    "            review=cols[2]\n",
    "            tokenized_review=nltk.word_tokenize(review.lower())\n",
    "            w_int=[]\n",
    "            for w in tokenized_review:\n",
    "                if w in vocab:\n",
    "                    w_int.append(vocab[w])\n",
    "                else:\n",
    "                    w_int.append(UNKNOWN_INDEX)\n",
    "            if len(w_int) < 549:\n",
    "                w_int.extend([PAD_INDEX] * (549 - len(w_int)))\n",
    "            if len(w_int) < 550:\n",
    "                data.append((w_int))\n",
    "                data_labels.append(labels[label])\n",
    "            if save_texts:\n",
    "                texts.append((idd, tokenized_review, label))\n",
    "    return data, data_labels, texts\n",
    "\n",
    "attn_embeddings=nn.Embedding.from_pretrained(embs)\n",
    "attn_train_x, attn_train_y, attn_train_texts=read_data_as_embeddings(training_file, glove_vocab, labels, True)\n",
    "attn_trainX, attn_trainY=get_batches(attn_train_x, attn_train_y, torch.LongTensor, 1)\n",
    "attn_dev_x, attn_dev_y, _=read_data_as_embeddings(dev_file, glove_vocab, labels)\n",
    "attn_devX, attn_devY=get_batches(attn_dev_x, attn_dev_y, torch.LongTensor, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to implement that as part of a model.  Here we're going to embed attention within a larger model.  For an input document of, say, $20$ words, each represented by a $100$-dimesional embedding, the input to attention is a $20 \\times 100$ matrix; the output from attention is also a $20 \\times 100$ matrix.  In this larger model, we're going to average those output embeddings to generate a final document that's a single $100$-dimensional vector; pass through a fully-connected dense layer to make a prediction.\n",
    "\n",
    "Test yourself before proceeding (not graded, but always remind yourself of this kind of things): In our GloVe representation of movie review data, each padded review has ? tokens, represented by a ?-dimensional GloVe embedding? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F0Lb0DRIUHOx"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    # BEGIN SOLUTION\n",
    "    raise NotImplementedError\n",
    "    # END SOLUTION\n",
    "\n",
    "class AttentionClassifier(nn.Module):\n",
    "    def __init__(self, params, pretrained_embeddings):\n",
    "        super().__init__()\n",
    "        self.seq_len=params[\"max_seq_len\"]\n",
    "        self.num_labels=params[\"label_length\"]\n",
    "        \n",
    "        self.query_key_size=params[\"query_key_size\"]\n",
    "        self.embeddings=nn.Embedding.from_pretrained(pretrained_embeddings)\n",
    "        self.input_embedding_size=self.embeddings.weight.data.shape[1]\n",
    "        self.attention=Attention(self.input_embedding_size, self.query_key_size)\n",
    "    \n",
    "        self.softmax=nn.Softmax(dim=1)\n",
    "        self.fc=nn.Linear(self.seq_len, params[\"label_length\"])\n",
    "    \n",
    "    def forward(self, input): \n",
    "        x=self.embeddings(input)\n",
    "        x=self.attention(x)\n",
    "        x=x.mean(1)\n",
    "        x=self.fc(x)\n",
    "        return x.squeeze()\n",
    "\n",
    "    def evaluate(self, x, y):\n",
    "        self.eval()\n",
    "        corr=0.\n",
    "        total=0.\n",
    "        with torch.no_grad():\n",
    "            for x, y in zip(x, y):\n",
    "                x, y=x.to(device), y.to(device)\n",
    "                x=x[0]\n",
    "                y_pred=self.forward(x)\n",
    "                prediction=torch.argmax(y_pred)\n",
    "                if prediction == y:\n",
    "                    corr += 1.\n",
    "                total+=1                          \n",
    "        return corr/total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V7sAsflHaXdB"
   },
   "outputs": [],
   "source": [
    "attnmodel=AttentionClassifier(\n",
    "    params={\n",
    "        \"max_seq_len\": 549, \n",
    "        \"label_length\": len(labels),\n",
    "        \"query_key_size\": 64\n",
    "    },\n",
    "    pretrained_embeddings=embs\n",
    ").to(device)\n",
    "\n",
    "optimizer=torch.optim.Adam(attnmodel.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "cross_entropy=nn.CrossEntropyLoss()\n",
    "losses=[]\n",
    "\n",
    "num_epochs=15\n",
    "best_dev_acc = 0.\n",
    "patience=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, you can execute the code to self-check, but you don't need to finish training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 609
    },
    "id": "3_UJSBJ86j5f",
    "outputId": "020527c5-21f0-405e-ecf6-5d251f29b580"
   },
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    attnmodel.train()\n",
    "\n",
    "    for x, y in zip(attn_trainX, attn_trainY):\n",
    "        x=x[0]\n",
    "        x, y=x.to(device), y.to(device)\n",
    "        y_pred=attnmodel.forward(x)\n",
    "        loss=cross_entropy(y_pred.view(-1, attnmodel.num_labels), y.view(-1))\n",
    "\n",
    "        losses.append(loss.item()) \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    dev_accuracy=attnmodel.evaluate(attn_devX, attn_devY)\n",
    "   \n",
    "    # check if the dev accuracy is the best seen so far; save the model if so\n",
    "    print(\"Epoch %s, dev accuracy: %.3f\" % (epoch, dev_accuracy))\n",
    "    if dev_accuracy > best_dev_acc:\n",
    "        torch.save(attnmodel.state_dict(), 'best-attnmodel-parameters.pt')\n",
    "        best_dev_acc = dev_accuracy\n",
    "        patience_counter=0\n",
    "        \n",
    "    patience_counter+=1\n",
    "    if patience_counter >= patience:\n",
    "        print(\"Stopping training; no improvement on dev data after %s epochs\" % patience)\n",
    "        break\n",
    "    \n",
    "attnmodel.load_state_dict(torch.load('best-attnmodel-parameters.pt'))\n",
    "print(\"\\nBest Performing Model achieves dev accuracy of : %.3f\" % (best_dev_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Congrats! You're officially done with this homework -- be sure to check the \"How to submit\" section in the PDF."
   ]
  }
 ],
 "metadata": {
    "colab": {
     "collapsed_sections": [],
     "name": "HW2",
     "provenance": [],
     "toc_visible": true
    },
    "kernelspec": {
     "display_name": "Python 3 (ipykernel)",
     "language": "python",
     "name": "python3"
    },
    "language_info": {
     "codemirror_mode": {
      "name": "ipython",
      "version": 3
     },
     "file_extension": ".py",
     "mimetype": "text/x-python",
     "name": "python",
     "nbconvert_exporter": "python",
     "pygments_lexer": "ipython3",
     "version": "3.8.8"
    }
   },
   "nbformat": 4,
   "nbformat_minor": 1
  }
  