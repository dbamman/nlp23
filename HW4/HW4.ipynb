{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/dbamman/nlp23/blob/main/HW4/HW4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tyClcLuqjKb8"
   },
   "source": [
    "# Homework 4: Transition-Based Dependency Parser\n",
    "\n",
    "**Due Tuesday March 14th, 2023 at 11:59pm**\n",
    "\n",
    "In this homework, you will be implementing components of a transition-based dependency parser.\n",
    "\n",
    "Before diving into any code, please read through the associated [PDF](https://github.com/dbamman/nlp23/blob/main/HW4/HW4.pdf) for an overview of the assignment and specific instructions on how to submit.\n",
    "\n",
    "As in prior homeworks, please don't remove the `BEGIN / END SOLUTION` flags, and type your logic *entirely* within them (including any helper functions).\n",
    "\n",
    "The graded deliverables for this assignment are found in Questions: [**1, 2.a, 2.b, 2.c, 3**]\n",
    "\n",
    "The *optional* Bonus sections do not contain graded deliverables. They allow you to further explore how a transition-based parser can be configured and utilized for a real-world task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njKMNfHmjKcR"
   },
   "source": [
    "## Question 1. Checking for Projectivity\n",
    "In this question, you are supposed to implement the `is_projective` function below.\n",
    "* A tree structure is said to be [projective](https://en.wikipedia.org/wiki/Discontinuity_(linguistics)) if there are no crossing dependency edges and/or projection lines. \n",
    "* The function should take a sentence as input and return `True` if and only if the tree is projective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fME7S_whjKcT"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "objective: Determine if a list of 5-element tuples represents a sentence with 'projective' dependency tree\n",
    "\n",
    "params: tokens is a list of (idd, tok, pos, head, lab) for a sentence\n",
    "return True if and only if the sentence has a projective dependency tree\n",
    "\"\"\"\n",
    "# instructions\n",
    "# there is more than one way to carry this out correctly. \n",
    "# We recommend the following procedure:\n",
    "# Part 1\n",
    "# Iterate over the tuples and store their relevant components a dictionary\n",
    "# with the tuples' id (\"idd\") as the key and its head (\"head\") as the value\n",
    "# this dictionary should thus contain dependency relations in the form {idd: head}\n",
    "# Part 2\n",
    "# For every dependency relation in that dictionary, extract the head\n",
    "# establish the left and right bounds for your dependency-path search, with\n",
    "# the left bound being: 1 + the smaller of (head, idd) and \n",
    "# the right bound, conversely, being the bigger of (head, idd)\n",
    "# Part 3\n",
    "# Check if every word in the tree is reachable by following the path of dependencies\n",
    "# complete the reachable method\n",
    "\n",
    "# hints\n",
    "# -the left bound gets a 1 added to it because you don't need to check for crossing edges \n",
    "#   when you have a token that's directly besides its head\n",
    "# - a token with '0' value for its head denotes that it is the root of the sentence\n",
    "# -if any word in the tree is not reachable, this means there is non-projectivity\n",
    "#   which is when your method needs to return False.\n",
    "# -otherwise, return True!\n",
    "# -you are welcome to remove any and all of our starter code and create your own logic for this\n",
    "# if you do so, please *do not* remove the BEGIN / END Solution flags!\n",
    "def is_projective(tokens):\n",
    "\n",
    "    # BEGIN SOLUTION\n",
    "    def reachable(i, head, heads): # populate this (Part 3)\n",
    "        pass\n",
    "    \n",
    "    heads = {} # populate this (Part 1)\n",
    "\n",
    "    for dep_to_check in heads: # populate this (Part 2)\n",
    "        \n",
    "        for i in range(left + 1, right):\n",
    "            if not reachable(i, head, heads):\n",
    "              return False\n",
    "            \n",
    "    return True\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iEe4jVsst2ZU"
   },
   "source": [
    "### Check `is_projective`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ccPr3IevJFd8",
    "outputId": "50a257ce-64db-467f-b81b-e148510115f2"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "objective: Sanity check the is_projective() function\n",
    "\"\"\"\n",
    "def sanity_check_is_projective():\n",
    "\n",
    "    # setup for checking projectivity --\n",
    "    # to test the method, input a projective sentence from the CONLL dataset\n",
    "    # e.g. \"From the AP comes this story:\" should be projective\n",
    "    proj_toks = [(1, 'From', 'IN', 3, 'case'), \n",
    "                  (2, 'the', 'DT', 3, 'det'), \n",
    "                  (3, 'AP', 'NNP', 4, 'obl'), \n",
    "                  (4, 'comes', 'VBZ', 0, 'root'), \n",
    "                  (5, 'this', 'DT', 6, 'det'), \n",
    "                  (6, 'story', 'NN', 4, 'nsubj'), \n",
    "                  (7, ':', ':', 4, 'punct')]\n",
    "    \n",
    "    assert is_projective(proj_toks) == True\n",
    "    \n",
    "    # test the method by inputting a non-projective sentence\n",
    "    # \"I saw a man today who is tall\" should not be projective\n",
    "    non_proj_toks = [(1, 'I', 'PRP', 2, 'nsubj'), \n",
    "                      (2, 'saw', 'VBD', 0, 'root'), \n",
    "                      (3, 'a', 'DT', 4, 'det'), \n",
    "                      (4, \"man\", 'NN', 2, 'obj'), \n",
    "                      (5, 'today', 'NN', 2, 'nmod'), \n",
    "                      (6, 'who', 'WP', 8, 'nsubj'), \n",
    "                      (7, 'is', 'VBZ', 8, 'cop'), \n",
    "                      (8, 'tall', 'JJ', 4, 'acl:relcl')]\n",
    "    assert is_projective(non_proj_toks) == False\n",
    "    print(\"You have cleared the sanity check for is_projective()!\")\n",
    "    \n",
    "sanity_check_is_projective()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eCZUcO60jKcW"
   },
   "source": [
    "## Question 2.a Shift Operation\n",
    "Implement the first helper function `perform_shift` to achieve the SHIFT operation.\n",
    "* The SHIFT Operation removes the word from the front of the input buffer and pushes it onto stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CFzJksVqjKcX"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "objective: perform the SHIFT operation\n",
    "\n",
    "params:\n",
    "    - wbuffer: input buffer\n",
    "    - stack: what we're buliding our parse on\n",
    "    - arcs: our dependency parse for a single token (containing the dependency relation, head-token ID, its own token ID)\n",
    "    - configurations: state of the parse at a given decision point\n",
    "    - gold_transitions: the operations we're applying at each decision point\n",
    "\"\"\"\n",
    "\n",
    "# instructions\n",
    "# 1. update configurations\n",
    "# 2. update gold_transitions\n",
    "# 3. remove word from front of buffer and push it onto stack\n",
    "# hints\n",
    "# -this can be completed in just a few lines.\n",
    "# -we have provided the code for updating configurations, so you just need to\n",
    "# update gold_transitions, then the stack and wbuffer accordingly\n",
    "# -since this is a SHIFT operation, arcs does not come into play because no head-dependent relationships are being asserted\n",
    "# -the .pop() operation removes an item at the given index from a list\n",
    "# defaulting to the last element in the list when no index is provided\n",
    "\n",
    "def perform_shift(wbuffer, stack, arcs, configurations, gold_transitions):    \n",
    "    \n",
    "    # BEGIN SOLUTION\n",
    "    # update configurations (Part 1)\n",
    "    configurations.append((list(wbuffer), list(stack), list(arcs)))\n",
    "\n",
    "    # update gold_transitions (Part 2)\n",
    "\n",
    "    # remove word from front of buffer and push it onto stack (Part 3)\n",
    "    \n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZG6ELNA1tzOt"
   },
   "source": [
    "### Check `perform_shift`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nt_K4zkCJRcV",
    "outputId": "53d7c860-bfb7-4197-8e75-e7619ae67b6a"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "objective: Sanity check the perform_shift() function\n",
    "\"\"\"  \n",
    "\n",
    "def sanity_check_perform_shift():\n",
    "\n",
    "    # setup for performing SHIFT\n",
    "    wbuffer = [3, 2, 1]\n",
    "    stack = [0]\n",
    "    arcs = []\n",
    "    configurations = []\n",
    "    gold_transitions = []\n",
    "\n",
    "    # Perform SHIFT by invoking student-function\n",
    "    perform_shift(wbuffer, stack, arcs, configurations, gold_transitions)\n",
    "\n",
    "    # validate outputs after performing SHIFT\n",
    "    assert wbuffer == [3, 2], \"The result for wbuffer is not correct\"\n",
    "    assert stack == [0, 1], \"The result for stack is not correct\"\n",
    "    assert arcs == [], \"The result for arcs is not correct\"\n",
    "    assert configurations == [([3, 2, 1], [0], [])], \"The result for configurations is not correct\"\n",
    "    assert gold_transitions == ['SHIFT'], \"The result for gold_transitions is not correct\"\n",
    "    print(\"You cleared the sanity check for perform_shift().\")\n",
    "    \n",
    "sanity_check_perform_shift()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JxqIrNnzjKcb"
   },
   "source": [
    "## Question 2.b ARC Operations\n",
    "Implement the second helper function `perform_arc` to achieve the ARC operation.\n",
    "\n",
    "* LEFT-ARC (label): assert relation between head at $stack_1$ and dependent at $stack_2$: remove $stack_2$\n",
    "* RIGHT-ARC (label): assert relation between head at $stack_2$ and dependent at $stack_1$; remove $stack_1$ \n",
    "\n",
    "Your addition to `gold_transitions` should be formatted as follows (case-sensitive): DIRECTION+ARC+_+dependency_label\n",
    "\n",
    "so if you are performing a right arc and `dep_label` is `punct` the output of this function is:\n",
    "\n",
    "`RIGHTARC_punct`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NW9_Y2fyjKcc"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "objective: perform LEFTARC_ and RIGHTARC_ operations\n",
    "\n",
    "params:\n",
    "    - direction: {\"LEFT\", \"RIGHT\"}\n",
    "    - dep_label: label for the dependency relations\n",
    "    - wbuffer: input buffer\n",
    "    - stack: what we're buliding our parse on\n",
    "    - arcs: our dependency parse for a single token (containing the dependency relation, head-token ID, its own token ID)\n",
    "    - configurations: state of the parse at a given decision point\n",
    "    - gold_transitions: the operations we're applying at each decision point\n",
    "\"\"\"\n",
    "\n",
    "# instructions\n",
    "# 1. update configurations\n",
    "# 2. update transitions\n",
    "# 3. use the first 2 elements on the stack to create a (head, child) pairing, based on the type of shift we're doing\n",
    "# 4. update arcs\n",
    "# 5. update the stack\n",
    "# hints\n",
    "# -updating configurations is identical to the perform_shift function\n",
    "# -be sure the information you're inserting into gold_transitions is formatted properly (see above)\n",
    "# -be sure that the orientation of the (head,child) pairing is correct\n",
    "\n",
    "def perform_arc(direction, dep_label, wbuffer, stack, arcs, configurations, gold_transitions):\n",
    "\n",
    "    # BEGIN SOLUTION\n",
    "    # update configurations (Part 1)\n",
    "    \n",
    "    # update transitions (Part 2)\n",
    "\n",
    "    # setup head and child items (Part 3)\n",
    "\n",
    "    # update arcs (Part 4)\n",
    "\n",
    "    # update stack (Part 5)\n",
    "    pass\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P1LhHrlwuBe5"
   },
   "source": [
    "### check `perform_arc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YbvwB7kkJV95",
    "outputId": "df68de05-be12-48ca-9e19-1b35bbb27fda"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sanity check for the function perform_arc()\n",
    "\"\"\"\n",
    "def sanity_check_perform_arc():\n",
    "\n",
    "    # setup for perfomring perform ARC\n",
    "    direction = 'RIGHT'\n",
    "    dep_label = 'punct'\n",
    "    wbuffer = [5, 4, 3]\n",
    "    stack = [0, 1, 2]\n",
    "    arcs = []\n",
    "    configurations = [([5, 4, 3, 2, 1], [0], []), \n",
    "                      ([5, 4, 3, 2], [0, 1], [])]\n",
    "    gold_transitions = ['SHIFT', 'SHIFT']\n",
    "\n",
    "    # Perform ARC by invoking student-function \n",
    "    perform_arc(direction, dep_label, wbuffer, stack, arcs, configurations, gold_transitions)\n",
    "\n",
    "    # Validate outputs after performng ARC\n",
    "    assert wbuffer == [5, 4, 3], \"The result for wbuffer is not correct\"\n",
    "    assert stack == [0, 1], \"The result for stack is not correct\"\n",
    "    assert arcs == [('punct', 1, 2)], \"The result for arcs is not correct\"\n",
    "    assert configurations == [([5, 4, 3, 2, 1], [0], []), \n",
    "                              ([5, 4, 3, 2], [0, 1], []), \n",
    "                              ([5, 4, 3], [0, 1, 2], [])], \\\n",
    "            \"The result for configurations is not correct\"\n",
    "    assert gold_transitions == ['SHIFT', 'SHIFT', 'RIGHTARC_punct'], \"The result for gold_transitions is not correct\"\n",
    "    print(\"You cleared the sanity check for perform_arc()!\")\n",
    "\n",
    "sanity_check_perform_arc()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mmDSyQpqjKcg"
   },
   "source": [
    "## Question 2.c Tree to Actions\n",
    "Now, since we have implemented the helper functions, let's use them to complete `tree_to_actions`.\n",
    "\n",
    "`tree_to_actions` takes `wbuffer`, `stack`, `arcs` and `deps` as input, then returns the configuration of the parser and action for the parser (`configurations` and `gold_transitions` respectively).\n",
    "\n",
    "Parts of this method have been filled in for you -- your job is to fill in how the right arc transitions should be invoked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9HeTVArDjKci"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "objective: based on inputs, return the correct configurations and actions for the parser.\n",
    "\n",
    "params:\n",
    "wbuffer: a list of word indices; the top of buffer is at the end of the list\n",
    "stack: a list of word indices; the top of buffer is at the end of the list\n",
    "arcs: a list of (label, head, dependent) tuples\n",
    "deps: the existing dependency tree\n",
    "\n",
    "Return configurations and gold_transitions\n",
    "\"\"\"\n",
    "\n",
    "# instructions\n",
    "# Initialize return lists\n",
    "# Check for simple cases (that we've translated the tree entirely, or that we need to get more words onto the stack)\n",
    "# Check for left-arc, if yes carry out\n",
    "# Check for right-arc, elif yes, carry out: your job is to fill in this portion of the code!\n",
    "# else, perform a shift\n",
    "# hints\n",
    "# -checking against conditions for the right arc operation is fairly similar to the left arc check\n",
    "#  with the order of stack1/stack2 swapped\n",
    "# -to check if all the dependents of s1 have been assigned, you can\n",
    "#  look for the presence of its children in dep_arcs\n",
    "# -don't forget to update dep_arc after you called your perform_arc function\n",
    "\n",
    "def tree_to_actions(wbuffer, stack, arcs, deps):\n",
    "\n",
    "    # Initialize return lists\n",
    "    \n",
    "    # A list of 3-element tuples of lists\n",
    "    # [(wbuffer1, stack1, arcs1), (wbuffer2, stack2, arcs2), ...]\n",
    "    # Keeps track of the states at each step\n",
    "    configurations=[]\n",
    "\n",
    "    # gold_transitions:\n",
    "    # A list of action strings, e.g [\"SHIFT\", \"LEFTARC_nsubj\"]\n",
    "    # Keeps track of the actions at each step\n",
    "    gold_transitions=[]\n",
    "\n",
    "    # Keeps track of the dependents of each word in the tree\n",
    "    # that have already been assigned\n",
    "    dep_arcs = {}\n",
    "    \n",
    "    while len(wbuffer) >= 0:\n",
    "        # Check for base-cases\n",
    "\n",
    "        # firstly, check if we have translated all the tree to transition instructions\n",
    "        if len(wbuffer) == 0 and len(stack) == 1 and stack[0] == 0:\n",
    "            return configurations, gold_transitions\n",
    "\n",
    "        # also, if there are fewer than 2 words on the stack\n",
    "        # and more than 0 left on the buffer, \n",
    "        # we need to perform a shift operation\n",
    "        if len(stack) < 2 and len(wbuffer) > 0:\n",
    "            # shift operations\n",
    "            perform_shift(wbuffer, stack, arcs, configurations, gold_transitions)\n",
    "            continue\n",
    "\n",
    "        # grab s1 and s2\n",
    "        stack1 = stack[-1]\n",
    "        stack2 = stack[-2]\n",
    "\n",
    "        # check against conditions for left arc operation\n",
    "        if stack1 in deps and (stack1, stack2) in deps[stack1]:\n",
    "            # perform left arc\n",
    "            perform_arc(\"LEFT\", deps[stack1][(stack1, stack2)], wbuffer, stack, arcs, configurations, gold_transitions)\n",
    "            # update dep_arcs\n",
    "            dep_arcs[stack2] = 1\n",
    "\n",
    "        # BEGIN SOLUTION\n",
    "  \n",
    "        # END SOLUTION\n",
    "\n",
    "        # perform shift\n",
    "        else:\n",
    "            perform_shift(wbuffer, stack, arcs, configurations, gold_transitions)\n",
    "    \n",
    "    return configurations, gold_transitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jf_7-OH7me5b"
   },
   "source": [
    "### check `tree_to_actions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fYpLgArZJb1X",
    "outputId": "e494de28-1697-41c3-b4da-ef9ff42a148b"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "objective: sanity check the tree_to_actions() function\n",
    "\"\"\"\n",
    "def sanity_check_tree_to_actions():\n",
    "\n",
    "    # Setup for invoking tree_to_actions \n",
    "    wbuffer = [9, 8, 7, 6, 5, 4, 3, 2, 1]\n",
    "    stack = [0]\n",
    "    arcs = []\n",
    "    deps = {5: {(5, 9): 'punct', (5, 8): 'obl', (5, 4): 'advmod', (5, 3): 'aux:pass', (5, 2): 'nsubj:pass'},\n",
    "            8: {(8, 7): 'det', (8, 6): 'case'}, 0: {(0, 5): 'root'}, 2: {(2, 1): 'nmod:poss'}}\n",
    "\n",
    "    tree_to_actions(wbuffer, stack, arcs, deps)\n",
    "\n",
    "    # After tree_to_actions\n",
    "    assert wbuffer == [], \"The result for wbuffer is not correct\"\n",
    "    assert stack == [0], \"The result for stack is not correct\"\n",
    "    assert arcs == [('nmod:poss', 2, 1), ('advmod', 5, 4), ('aux:pass', 5, 3), ('nsubj:pass', 5, 2), \n",
    "                     ('det', 8, 7), ('case', 8, 6), ('obl', 5, 8), ('punct', 5, 9), ('root', 0, 5)], \\\n",
    "                        \"The result for arcs is not correct\"\n",
    "    assert deps == {5: {(5, 9): 'punct', (5, 8): 'obl', (5, 4): 'advmod', (5, 3): 'aux:pass', (5, 2): 'nsubj:pass'}, \n",
    "                     8: {(8, 7): 'det', (8, 6): 'case'}, 0: {(0, 5): 'root'}, 2: {(2, 1): 'nmod:poss'}}, \\\n",
    "                    \"The result for deps is not correct\"\n",
    "    print(\"You cleared the sanity check for tree_to_actions()!\")   \n",
    "     \n",
    "sanity_check_tree_to_actions()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LB8BWcevs1qU"
   },
   "source": [
    "## Question 3. Dependency grammar vs. Phrase-structure grammar [written]\n",
    "\n",
    "In the space below, please explain some **key differences** between the phrase-structure grammars and dependency grammars (~150 words). \n",
    "\n",
    "We aren't looking for an exhaustive list, rather, a sufficiently detailed explanation about the situational advantages and major differences that each of these different formalisms offers. See the assignment writeup for tips on getting started and directions to head for further information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RWEX47JwtVnN"
   },
   "source": [
    "### Q3 response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TOade2BW9P7g"
   },
   "source": [
    "**type q3 response here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-oZIqUAjs3d"
   },
   "source": [
    "## Bonus: Tree Parsing with Predictions\n",
    "\n",
    "**Note**: nothing in this section, or the subsequent \"bonus\" section, is graded. \n",
    "\n",
    "As a follow-up, the `action_to_tree` method will update the dependency tree based on the action predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O8KMc8I8zDJO"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pFTfAwLWjKcm"
   },
   "outputs": [],
   "source": [
    "def is_valid(stack, wbuffer, action):\n",
    "\n",
    "    if action == \"SHIFT\" and len(wbuffer) > 0:\n",
    "        return True\n",
    "    if action.startswith(\"RIGHTARC\") and len(stack) > 1 and stack[-1] != 0:\n",
    "        return True\n",
    "    if action.startswith(\"LEFTARC\") and len(stack) > 1 and stack[-2] != 0:\n",
    "        return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RH_vzMzRjKcp"
   },
   "outputs": [],
   "source": [
    "def action_to_tree(tree, predictions, wbuffer, stack, arcs, reverse_labels):\n",
    "\n",
    "    sorted_probs = np.argsort(-predictions, kind='quicksort')[0]\n",
    "\n",
    "    for i in range(len(sorted_probs)):\n",
    "        action = reverse_labels[sorted_probs[i]]\n",
    "        if is_valid(stack, wbuffer, action):\n",
    "            if action == \"SHIFT\":\n",
    "                stack.append(wbuffer.pop())\n",
    "\n",
    "            elif len(stack) > 1 and action.startswith(\"RIGHTARC\"):\n",
    "                tree[stack[-1]] = (stack[-2], re.sub(\"RIGHTARC_\", \"\", action))\n",
    "                stack.pop()\n",
    "\n",
    "            elif len(stack) > 1 and action.startswith(\"LEFTARC\"):\n",
    "                tree[stack[-2]] = (stack[-1], re.sub(\"LEFTARC_\", \"\", action))\n",
    "                stack.pop(-2)\n",
    "\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJsWiPY5jKcu"
   },
   "source": [
    "\n",
    "## Bonus: Neural Dependency Parser\n",
    "Now since you have the configuration $x$ and action $y$, we can now train a supervised model to predict an action $y$ given a configuration $x$. We are using a simplified version of the model from [A Fast and Accurate Dependency Parser using Neural Networks](https://nlp.stanford.edu/pubs/emnlp2014-depparser.pdf).\n",
    "\n",
    "* This model is alreadly implemented for you, please `train` the model, and report the evaluation and test results by calling the function `evaluate` and `test`\n",
    "* To run the code for this section, you'll need to import pytorch + related libraries, switch your runtime to GPU, and pull in GLoVE embeddings -- this is provided for you in the next few cells as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S4pBoWS6lIVG"
   },
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ASKLicdki75"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tp4oRVxCkjyi",
    "outputId": "d7a57bc3-85c2-4c33-c8be-e5c09a613fa5"
   },
   "outputs": [],
   "source": [
    "# if this cell prints \"Running on cpu\", switch runtime environments\n",
    "# go to Runtime > Change runtime type > Hardware accelerator > GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Running on {}\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AXbaGhkZoEAY"
   },
   "source": [
    "Again, we will still be using [GloVe](https://nlp.stanford.edu/projects/glove/) pretrained word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4eMEXXVpoDTx",
    "outputId": "0e61e10c-7b44-4f3a-e543-578dcd6ae64a"
   },
   "outputs": [],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip glove*.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, pull in Universal Dependency data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/dbamman/nlp23/main/HW4/train.projective.short.conll\n",
    "!wget https://raw.githubusercontent.com/dbamman/nlp23/main/HW4/dev.projective.conll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0gda3EYQlOdP"
   },
   "source": [
    "#### Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vlx_IRrJjKcv"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Return pairs of configurations + gold transitions (actions)\n",
    "from training data\n",
    "configuration = a list of tuple of:\n",
    "    - buffer (top of buffer is at the end of the list)\n",
    "    - stack (top of buffer is at the end of the list)\n",
    "    - arcs (a list of (label, head, dependent) tuples)\n",
    "gold transitions = a list of actions, e.g. SHIFT\n",
    "\"\"\"\n",
    "def get_oracle(toks):\n",
    "\n",
    "    stack = [] # stack\n",
    "    arcs = [] # existing list of arcs\n",
    "    wbuffer = [] # input buffer\n",
    "\n",
    "    # deps is a dictionary of head: dependency relations, where\n",
    "    # dependency relations is a dictionary of the (head, child): label\n",
    "    # deps = {head1:{\n",
    "    #               (head1, child1):dependency_label1,\n",
    "    #               (head1, child2):dependency_label2\n",
    "    #              }\n",
    "    #         head2:{\n",
    "    #               (head2, child3):dependency_label3,\n",
    "    #               (head2, child4):dependency_label4\n",
    "    #              }\n",
    "    #         }\n",
    "    deps = {}\n",
    "\n",
    "    # ROOT\n",
    "    stack.append(0)\n",
    "\n",
    "    # initialize variables\n",
    "    for position in reversed(toks):\n",
    "        (idd, _, _, head, lab) = position\n",
    "\n",
    "        dep = (head, idd)\n",
    "        if head not in deps:\n",
    "            deps[head] = {}\n",
    "        deps[head][dep] = lab\n",
    "\n",
    "        wbuffer.append(idd)\n",
    "\n",
    "    # configurations:\n",
    "    # A list of (wbuffer, stack, arcs)\n",
    "    # Keeps tracks of the states at each step\n",
    "    # gold_transitions:\n",
    "    # A list of action strings [\"SHIFT\", \"LEFTARC_nsubj\"]\n",
    "    # Keeps tracks of the actions at each step\n",
    "    configurations, gold_transitions = tree_to_actions(wbuffer, stack, arcs, deps)\n",
    "    return configurations, gold_transitions\n",
    "\n",
    "def featurize_configuration(configuration, tokens, postags, vocab, pos_vocab):\n",
    "    \n",
    "    \"\"\"\n",
    "    Given configurations of the stack, input buffer and arcs,\n",
    "    words of the sentence and POS tags of the words,\n",
    "    return some features\n",
    "\n",
    "    The current features are the word ID and postag ID at the \n",
    "    first three positions of the stack and buffer.\n",
    "    \"\"\"\n",
    "\n",
    "    def get_id(word, vocab):\n",
    "        word=word.lower()\n",
    "        if word in vocab:\n",
    "            return vocab[word]\n",
    "        return vocab[\"<unk>\"]\n",
    "\n",
    "    wbuffer, stack, arcs = configuration\n",
    "\n",
    "    word_features=[]\n",
    "    pos_features=[]\n",
    "\n",
    "    if len(stack) > 0: \n",
    "        word_features.append(get_id(tokens[stack[-1]], vocab))\n",
    "        pos_features.append(get_id(postags[stack[-1]], pos_vocab))\n",
    "    else: \n",
    "        word_features.append(get_id(\"<NONE>\", vocab))\n",
    "        pos_features.append(get_id(\"<NONE>\", pos_vocab))\n",
    "\n",
    "    if len(stack) > 1: \n",
    "        word_features.append(get_id(tokens[stack[-2]], vocab))\n",
    "        pos_features.append(get_id(postags[stack[-2]], pos_vocab))\n",
    "    else: \n",
    "        word_features.append(get_id(\"<NONE>\", vocab))\n",
    "        pos_features.append(get_id(\"<NONE>\", pos_vocab))\n",
    "\n",
    "    if len(stack) > 2: \n",
    "        word_features.append(get_id(tokens[stack[-3]], vocab))\n",
    "        pos_features.append(get_id(postags[stack[-3]], pos_vocab))\n",
    "    else: \n",
    "        word_features.append(get_id(\"<NONE>\", vocab))\n",
    "        pos_features.append(get_id(\"<NONE>\", pos_vocab))\n",
    "\n",
    "    if len(wbuffer) > 0: \n",
    "        word_features.append(get_id(tokens[wbuffer[-1]], vocab))\n",
    "        pos_features.append(get_id(postags[wbuffer[-1]], pos_vocab))\n",
    "    else: \n",
    "        word_features.append(get_id(\"<NONE>\", vocab))\n",
    "        pos_features.append(get_id(\"<NONE>\", pos_vocab))\n",
    "       \n",
    "    if len(wbuffer) > 1: \n",
    "        word_features.append(get_id(tokens[wbuffer[-2]], vocab))\n",
    "        pos_features.append(get_id(postags[wbuffer[-2]], pos_vocab))\n",
    "    else: \n",
    "        word_features.append(get_id(\"<NONE>\", vocab))\n",
    "        pos_features.append(get_id(\"<NONE>\", pos_vocab))\n",
    "\n",
    "    if len(wbuffer) > 2: \n",
    "        word_features.append(get_id(tokens[wbuffer[-3]], vocab))\n",
    "        pos_features.append(get_id(postags[wbuffer[-3]], pos_vocab))\n",
    "    else: \n",
    "        word_features.append(get_id(\"<NONE>\", vocab))\n",
    "        pos_features.append(get_id(\"<NONE>\", pos_vocab))\n",
    "\n",
    "    return word_features, pos_features\n",
    "\n",
    "\"\"\"\n",
    "Get configurations, gold_transitions from all sentences\n",
    "\"\"\"\n",
    "def get_oracles(filename, vocab, tag_vocab):\n",
    "\n",
    "    with open(filename) as f:\n",
    "        toks, tokens, postags = [], {}, {}\n",
    "        tokens[0] = \"<ROOT>\"\n",
    "        postags[0] = \"<ROOT>\"\n",
    "\n",
    "        # a list of all features for each transition step\n",
    "        word_feats = []\n",
    "        pos_feats = []\n",
    "        # a list of labels, e.g. SHIFT, LEFTARC_DEP_LABEL, RIGHTARC_DEP_LABEL\n",
    "        labels = []\n",
    "\n",
    "        for line in f:\n",
    "            cols = line.rstrip().split(\"\\t\")\n",
    "            \n",
    "            if len(cols) < 2: # at the end of each sentence\n",
    "                if len(toks) > 0:\n",
    "                    if is_projective(toks): # only use projective trees\n",
    "                        # get all configurations and gold standard transitions\n",
    "                        configurations, gold_transitions = get_oracle(toks)\n",
    "                        \n",
    "                        for i in range(len(configurations)):\n",
    "                            word_feat, pos_feat = featurize_configuration(configurations[i], tokens, postags, vocab, tag_vocab)\n",
    "                            label = gold_transitions[i]\n",
    "                            word_feats.append(word_feat)\n",
    "                            pos_feats.append(pos_feat)\n",
    "                            labels.append(label)\n",
    "\n",
    "                    # reset vars for the next sentence\n",
    "                    toks, tokens, postags = [], {}, {}\n",
    "                    tokens[0] = \"<ROOT>\"\n",
    "                    postags[0] = \"<ROOT>\"\n",
    "                    \n",
    "                continue\n",
    "\n",
    "            if cols[0].startswith(\"#\"):\n",
    "                continue\n",
    "\n",
    "            # construct the tuple for each word in the sentence\n",
    "            # for each word in the sentence\n",
    "            # idd: index of a word in a sentence, starting from 1\n",
    "            # tok: the word itself\n",
    "            # pos: pos tag for that word\n",
    "            # head: parent of the dependency\n",
    "            # lab: dependency relation label\n",
    "            idd, tok, pos, head, lab = int(cols[0]), cols[1], cols[4], int(cols[6]), cols[7]\n",
    "            toks.append((idd, tok, pos, head, lab))\n",
    "\n",
    "            # feature for training to predict the gold transition\n",
    "            tokens[idd], postags[idd] = tok, pos\n",
    "\n",
    "        return word_feats, pos_feats, labels\n",
    "\n",
    "def load_embeddings(filename):\n",
    "    # 0 idx is for padding\n",
    "    # 1 idx is for <UNK>\n",
    "    # 2 idx is for <NONE>\n",
    "    # 3 idx is for <ROOT>\n",
    "\n",
    "    # get the embedding size from the first embedding\n",
    "    vocab_size=4\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        for idx, line in enumerate(file):\n",
    "            if idx == 0:\n",
    "                word_embedding_dim=len(line.rstrip().split(\" \"))-1\n",
    "            vocab_size+=1\n",
    "\n",
    "    vocab={\"<pad>\":0, \"<unk>\":1, \"<none>\":2, \"<root>\":3}\n",
    "    print(f\"word_embedding_dim: {word_embedding_dim}, vocab_size: {vocab_size}\")\n",
    "\n",
    "    embeddings=np.zeros((vocab_size, word_embedding_dim))\n",
    "\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        for idx,line in enumerate(file):\n",
    "\n",
    "            if idx + 4 >= vocab_size:\n",
    "                break\n",
    "\n",
    "            cols=line.rstrip().split(\" \")\n",
    "            val=np.array(cols[1:])\n",
    "            word=cols[0]\n",
    "            embeddings[idx+4]=val\n",
    "            vocab[word]=idx+4\n",
    "\n",
    "    return torch.FloatTensor(embeddings), vocab\n",
    "\n",
    "class ShiftReduceParser(nn.Module):\n",
    "\n",
    "    def __init__(self, embeddings, hidden_dim, tagset_size, num_pos_tags, pos_embedding_dim):\n",
    "        super(ShiftReduceParser, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_labels=tagset_size\n",
    "\n",
    "        _, embedding_dim = embeddings.shape\n",
    "\n",
    "        self.input_size=embedding_dim*6 + pos_embedding_dim*6\n",
    "        \n",
    "        self.dropout_layer = nn.Dropout(p=0.25)\n",
    "\n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(embeddings)\n",
    "        self.pos_embeddings = nn.Embedding(num_pos_tags, pos_embedding_dim)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.W1 = nn.Linear(self.input_size, self.hidden_dim)\n",
    "        self.W2 = nn.Linear(self.hidden_dim, self.num_labels)\n",
    "\n",
    "    def forward(self, words, pos_tags, Y=None):\n",
    "        \n",
    "        words=words.to(device)\n",
    "        pos_tags=pos_tags.to(device)\n",
    "\n",
    "        if Y is not None:\n",
    "            Y=Y.to(device)\n",
    "\n",
    "        word_embeds = self.word_embeddings(words)\n",
    "        postag_embeds = self.pos_embeddings(pos_tags)\n",
    "\n",
    "        embeds=torch.cat((word_embeds, postag_embeds), 2)\n",
    "\n",
    "        embeds=embeds.view(-1, self.input_size)\n",
    "\n",
    "        embeds=self.dropout_layer(embeds)\n",
    "\n",
    "        hidden = self.W1(embeds)\n",
    "        hidden = self.tanh(hidden)\n",
    "        logits = self.W2(hidden)\n",
    "\n",
    "        if Y is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), Y.view(-1))\n",
    "            return loss\n",
    "        else:\n",
    "            return logits\n",
    "\n",
    "def get_batches(W, P, Y, batch_size):\n",
    "    batch_W=[]\n",
    "    batch_P=[]\n",
    "    batch_Y=[]\n",
    "\n",
    "    i=0\n",
    "    while i < len(W):\n",
    "        batch_W.append(torch.LongTensor(W[i:i+batch_size]))\n",
    "        batch_P.append(torch.LongTensor(P[i:i+batch_size]))\n",
    "        batch_Y.append(torch.LongTensor(Y[i:i+batch_size]))\n",
    "        i+=batch_size  \n",
    "\n",
    "    return batch_W, batch_P, batch_Y\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Train transition-based parser to predict next action (labels)\n",
    "given current configuration (featurized by word_feats and pos_feats)\n",
    "Return the classifier trained using Chen and Manning (2014), \"A Fast \n",
    "and Accurate Dependency Parser using Neural Networks\"\n",
    "\n",
    "\"\"\"\n",
    "def train(word_feats, pos_feats, labels, embeddings, vocab, postag_vocab, label_vocab):\n",
    "\n",
    "\n",
    "    # dimensionality of linear layer\n",
    "    HIDDEN_DIM=100\n",
    "    # dimensionality of POS embeddings\n",
    "    POS_EMBEDDING_SIZE=50\n",
    "\n",
    "    # batch size for training\n",
    "    BATCH_SIZE=32\n",
    "\n",
    "    # number of epochs to train for\n",
    "    NUM_EPOCHS=10\n",
    "\n",
    "    # learning rate for Adam optimizer\n",
    "    LEARNING_RATE=0.001\n",
    "\n",
    "    num_labels=[]\n",
    "    for i, y in enumerate(labels):\n",
    "        num_labels.append(label_vocab[y])\n",
    "\n",
    "    batch_W, batch_P, batch_Y = get_batches(word_feats, pos_feats, num_labels, BATCH_SIZE)\n",
    "\n",
    "    model = ShiftReduceParser(embeddings, HIDDEN_DIM, len(label_vocab), len(postag_vocab), POS_EMBEDDING_SIZE)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "\n",
    "        bigloss=0.\n",
    "        for b in range(len(batch_W)):\n",
    "            model.zero_grad()\n",
    "\n",
    "            loss = model.forward(batch_W[b], batch_P[b], Y=batch_Y[b])\n",
    "            bigloss+=loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"loss: {bigloss}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\"\"\"\n",
    "parse sentence with trained model and return correctness measure\n",
    "\"\"\"\n",
    "def parse(toks, model, vocab, tag_vocab, reverse_labels):\n",
    "\n",
    "    tokens, postags = {}, {}\n",
    "    tokens[0] = \"<ROOT>\"\n",
    "    postags[0] = \"<ROOT>\"\n",
    "\n",
    "    wbuffer, stack, arcs = [], [], []\n",
    "    stack.append(0)\n",
    "\n",
    "    for position in reversed(toks):\n",
    "\n",
    "        (idd, tok, pos, head, lab) = position\n",
    "        tokens[idd] = tok\n",
    "        postags[idd] = pos\n",
    "\n",
    "        # update buffer\n",
    "        wbuffer.append(idd)\n",
    "\n",
    "    tree = {}\n",
    "    while len(wbuffer) >= 0:\n",
    "        if len(wbuffer) == 0 and len(stack) == 0: break\n",
    "        if len(wbuffer) == 0 and len(stack) == 1 and stack[0] == 0: break\n",
    "\n",
    "        word_feats, pos_feats = (featurize_configuration((wbuffer, stack, arcs), tokens, postags, vocab, tag_vocab))\n",
    "\n",
    "       \n",
    "        predictions=model.forward(torch.LongTensor([word_feats]), torch.LongTensor([pos_feats]))\n",
    "\n",
    "        predictions=predictions.detach().cpu().numpy()\n",
    "\n",
    "        # your function will be called here\n",
    "        action_to_tree(tree, predictions, wbuffer, stack, arcs, reverse_labels)\n",
    "\n",
    "    return tree\n",
    "\n",
    "\"\"\"\n",
    "parse sentence with trained model and return correctness measure\n",
    "\"\"\"\n",
    "def parse_and_evaluate(toks, model, vocab, tag_vocab, reverse_labels):\n",
    "\n",
    "    heads, labels = {}, {}\n",
    "\n",
    "    for position in reversed(toks):\n",
    "        (idd, tok, pos, head, lab) = position\n",
    "\n",
    "        # keep track of gold standards for performance evaluation\n",
    "        heads[idd], labels[idd] = head, lab\n",
    "\n",
    "    tree = parse(toks, model, vocab, tag_vocab, reverse_labels)\n",
    "\n",
    "    # correct_unlabeled: total number of correct (head, child) dependencies\n",
    "    # correct_labeled: total number of correctly *labeled* dependencies\n",
    "    correct_unlabeled, correct_labeled, total = 0, 0, 0\n",
    "\n",
    "    for child in tree:\n",
    "        (head, label) = tree[child]\n",
    "        if head == heads[child]:\n",
    "            correct_unlabeled += 1\n",
    "            if label == labels[child]: correct_labeled += 1\n",
    "        total += 1\n",
    "\n",
    "    return [correct_unlabeled, correct_labeled, total]\n",
    "\n",
    "def get_label_vocab(labels):\n",
    "    tag_vocab={}\n",
    "    num_labels=[]\n",
    "    for i, y in enumerate(labels):\n",
    "        if y not in tag_vocab:\n",
    "            tag_vocab[y]=len(tag_vocab)\n",
    "        num_labels.append(tag_vocab[y])\n",
    "\n",
    "    reverse_labels=[None]*len(tag_vocab)\n",
    "    for y in tag_vocab:\n",
    "        reverse_labels[tag_vocab[y]]=y\n",
    "\n",
    "    return tag_vocab, reverse_labels\n",
    "\n",
    "\n",
    "def get_pos_tag_vocab(filename):\n",
    "    tag_vocab={\"<none>\":0, \"<unk>\":1}\n",
    "    with open(filename) as file:\n",
    "        for line in file:\n",
    "            cols=line.rstrip().split(\"\\t\")\n",
    "            if len(cols) < 3:\n",
    "                continue\n",
    "            pos=cols[4].lower()\n",
    "            if pos not in tag_vocab:\n",
    "                tag_vocab[pos]=len(tag_vocab)\n",
    "    return tag_vocab\n",
    "\n",
    "\"\"\"\n",
    "Evaluate the performance of a parser against gold standard\n",
    "\"\"\"\n",
    "def test(model, vocab, tag_vocab, reverse_labels):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    toks=[\"I\", \"bought\", \"a\", \"book\"]\n",
    "    pos=[\"NNP\", \"VBD\", \"DT\", \"NN\"]\n",
    "\n",
    "    data=[]\n",
    "    # put it in format parser expects\n",
    "    for i, tok in enumerate(toks):\n",
    "        data.append((i+1, tok, pos[i], \"_\", \"_\"))\n",
    "\n",
    "    tree=parse(data, model, vocab, tag_vocab, reverse_labels)\n",
    "\n",
    "    for child in sorted(tree.keys()):\n",
    "        (head, label) = tree[child]\n",
    "        headStr=\"<ROOT>\"\n",
    "        # child and head indexes start at 1; 0 denotes the <ROOT>\n",
    "        if head > 0: \n",
    "            headStr=toks[head-1]\n",
    "        \n",
    "        print(f\"{child, toks[child-1]} -> ({head, headStr}) {label}\")\n",
    "  \n",
    "\"\"\"\n",
    "Evaluate the performance of a parser against gold standard\n",
    "\"\"\"\n",
    "def evaluate(filename, model, vocab, tag_vocab, reverse_labels):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with open(filename) as f:\n",
    "        toks=[]\n",
    "        totals = np.zeros(3)\n",
    "        for line in f:\n",
    "            cols=line.rstrip().split(\"\\t\")\n",
    "\n",
    "            if len(cols) < 2: # end of a sentence\n",
    "                if len(toks) > 0:\n",
    "                    if is_projective(toks):\n",
    "                        tots = np.array(parse_and_evaluate(toks, model, vocab, tag_vocab, reverse_labels))\n",
    "                        totals += tots\n",
    "                        \n",
    "                    toks = []\n",
    "                continue\n",
    "\n",
    "            if cols[0].startswith(\"#\"):\n",
    "                continue\n",
    "\n",
    "            idd, tok, pos, head, lab = int(cols[0]), cols[1], cols[4], int(cols[6]), cols[7]\n",
    "            toks.append((idd, tok, pos, head, lab))\n",
    "        \n",
    "        print(f\"UAS: {totals[0]/totals[2]}, LAS: {totals[1]/totals[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ax1VrekKjKcy"
   },
   "source": [
    "### Train and evaluate the neural model\n",
    "\n",
    "* To reiterate, because you are not implementing the neural model or its training process, you will **not** be graded based on the performance of the neural model!\n",
    "\n",
    "* You are only graded based on the correctness of each of the implemented functions from Questions 1 and 2. \n",
    "\n",
    "* That said, as a way to check your programming work, if all the functions you wrote are implemented correctly, you should expect a **UAS** in a range of **[0.64, 0.67]**, and a **LAS** in a range of **[0.56, 0.59]** without changing the parameters of the neural model or the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9PweOgvvjKc0",
    "outputId": "7b09badb-4d94-461d-8909-10b5560782d7"
   },
   "outputs": [],
   "source": [
    "# load the embeddings file we downloaded\n",
    "embeddingsFile = \"glove.6B.50d.txt\"\n",
    "\n",
    "# load in trainFile and devFile\n",
    "trainFile = \"train.projective.short.conll\"\n",
    "devFile = \"dev.projective.conll\"\n",
    "\n",
    "embeddings, vocab=load_embeddings(embeddingsFile)\n",
    "pos_tag_vocab=get_pos_tag_vocab(trainFile)\n",
    "word_feats, pos_feats, labels = get_oracles(trainFile, vocab, pos_tag_vocab)\n",
    "\n",
    "label_vocab, reverse_labels=get_label_vocab(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lFN4U_1ojKc5",
    "outputId": "455fddb5-0e2b-4a58-dfee-ef316fc47970"
   },
   "outputs": [],
   "source": [
    "model = train(word_feats, pos_feats, labels, embeddings, vocab, pos_tag_vocab, label_vocab)\n",
    "evaluate(devFile, model, vocab, pos_tag_vocab, reverse_labels)\n",
    "test(model, vocab, pos_tag_vocab, reverse_labels)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HW4",
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
